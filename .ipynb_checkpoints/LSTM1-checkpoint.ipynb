{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from training & testing dataset\n",
    "training_csv_file = 'dataset_2020_training.csv'\n",
    "testing_csv_file = 'dataset_2020_testing1.csv'\n",
    "training_set = pd.read_csv(training_csv_file, header=0)\n",
    "testing_set = pd.read_csv(testing_csv_file, header=0)\n",
    "\n",
    "#The output is: knee/ankle angle/torque l/r\n",
    "out_num = 8\n",
    "\n",
    "#Get the label\n",
    "Label_df = pd.read_csv(training_csv_file, header=0, nrows = 1)\n",
    "Label= Label_df.columns.values\n",
    "output_label = np.array(Label[-out_num:])\n",
    "\n",
    "#Get the values and convert to float type\n",
    "training_values = training_set.values\n",
    "testing_values = testing_set.values\n",
    "time_train = training_values[:,0]\n",
    "time_test  = testing_values[:,0]\n",
    "training_data = training_values[:,1:].astype('float32')\n",
    "testing_data = testing_values[:,1:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(ndarray):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(ndarray)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get scaler for inputs/outputs\n",
    "X_scaler_training = get_scaler(training_data[:,:98])\n",
    "Y_scaler_training = get_scaler(training_data[:,-8:])\n",
    "X_scaler_testing = get_scaler(testing_data[:,:98])\n",
    "Y_scaler_testing = get_scaler(testing_data[:,-8:])\n",
    "\n",
    "\n",
    "X_train = X_scaler_training.transform(training_data[:,:98])\n",
    "Y_train = Y_scaler_training.transform(training_data[:,-8:])\n",
    "X_test = X_scaler_testing.transform(testing_data[:,:98])\n",
    "Y_test = Y_scaler_testing.transform(testing_data[:,-8:])\n",
    "\n",
    "#Get scaler for the whole training data\n",
    "train_data_scaler = get_scaler(training_data)\n",
    "test_data_scaler = get_scaler(testing_data)\n",
    "\n",
    "#Transform training data into scaled data and tensor data type\n",
    "train_data_normalized = train_data_scaler.transform(training_data)\n",
    "test_data_normalized =test_data_scaler.transform(testing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(A, timesteps):\n",
    "    return A.reshape(int(A.shape[0] / timesteps), timesteps, A.shape[1])\n",
    "\n",
    "#Return tupple containing inputs and labels\n",
    "def create_inout_sequences(input_data, size, out_num, time_steps):\n",
    "    seq = reshape(input_data[:,:size], time_steps)\n",
    "    out = reshape(input_data[:,-out_num:], time_steps)\n",
    "    return seq, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.56939244e-01,  7.11062193e-01,  4.09192979e-01,\n",
       "         1.07450366e-01, -5.53563714e-01,  7.01500356e-01,\n",
       "         5.65648794e-01,  4.27066684e-02],\n",
       "       [-6.78051233e-01,  7.79404044e-01,  5.25343478e-01,\n",
       "        -4.01988029e-02, -6.72574282e-01,  7.77232230e-01,\n",
       "         6.33226871e-01, -1.05120778e-01],\n",
       "       [-7.62396812e-01,  8.24274182e-01,  5.89141190e-01,\n",
       "        -1.71931505e-01, -7.54591703e-01,  8.27659547e-01,\n",
       "         6.53589964e-01, -1.95789218e-01],\n",
       "       [-8.07320595e-01,  8.48463893e-01,  6.36925519e-01,\n",
       "        -3.51417303e-01, -7.98710585e-01,  8.53516400e-01,\n",
       "         6.54876828e-01, -3.51654768e-01],\n",
       "       [-8.13657999e-01,  8.60693097e-01,  6.54800475e-01,\n",
       "        -4.82574940e-01, -8.03837299e-01,  8.60979736e-01,\n",
       "         6.32220268e-01, -5.01585841e-01],\n",
       "       [-7.80356884e-01,  8.62670898e-01,  6.19420826e-01,\n",
       "        -4.96664524e-01, -7.67527580e-01,  8.52330148e-01,\n",
       "         5.67536354e-01, -5.28172612e-01],\n",
       "       [-7.05393553e-01,  8.44888091e-01,  5.47699630e-01,\n",
       "        -4.31240082e-01, -6.89166307e-01,  8.23829234e-01,\n",
       "         4.86198306e-01, -4.87417102e-01],\n",
       "       [-5.88831067e-01,  8.02520752e-01,  4.44339216e-01,\n",
       "        -3.17683935e-01, -5.69055676e-01,  7.71836579e-01,\n",
       "         4.21741247e-01, -3.60556006e-01],\n",
       "       [-4.30692554e-01,  7.27441549e-01,  3.19305360e-01,\n",
       "        -1.45211935e-01, -4.07251835e-01,  6.81896508e-01,\n",
       "         2.93547213e-01, -1.44286394e-01],\n",
       "       [-2.31594443e-01,  6.08628273e-01,  5.42485714e-02,\n",
       "         8.55844021e-02, -2.04840183e-01,  5.45670569e-01,\n",
       "         2.09428072e-02,  1.63272619e-01],\n",
       "       [ 1.54614449e-04,  4.37458754e-01, -3.57268929e-01,\n",
       "         3.70382994e-01,  2.94506550e-02,  3.58753264e-01,\n",
       "        -3.09730411e-01,  4.95305777e-01],\n",
       "       [ 2.51327634e-01,  2.35927343e-01, -6.32676303e-01,\n",
       "         5.79616249e-01,  2.80932546e-01,  1.41518176e-01,\n",
       "        -4.87587422e-01,  6.88547254e-01],\n",
       "       [ 4.94673312e-01,  3.98510695e-02, -7.06878662e-01,\n",
       "         6.56009197e-01,  5.18931568e-01, -7.06242919e-02,\n",
       "        -5.47403038e-01,  7.43945718e-01],\n",
       "       [ 7.00899005e-01, -1.11531496e-01, -7.39466906e-01,\n",
       "         6.59382582e-01,  7.13588417e-01, -2.39833772e-01,\n",
       "        -5.92887998e-01,  6.89375281e-01],\n",
       "       [ 8.49679351e-01, -1.93062603e-01, -6.11851096e-01,\n",
       "         6.21005774e-01,  8.48834157e-01, -3.28465104e-01,\n",
       "        -5.44212759e-01,  5.30233502e-01],\n",
       "       [ 9.33025062e-01, -2.13918567e-01, -5.48408866e-01,\n",
       "         5.83273530e-01,  9.21841860e-01, -3.41141164e-01,\n",
       "        -4.88733858e-01,  4.51241255e-01],\n",
       "       [ 9.74897504e-01, -2.11151153e-01, -5.98715961e-01,\n",
       "         5.71254015e-01,  9.56764698e-01, -3.31867695e-01,\n",
       "        -4.60030973e-01,  5.21491647e-01],\n",
       "       [ 9.92575169e-01, -2.12029338e-01, -5.99507928e-01,\n",
       "         6.11360192e-01,  9.76594269e-01, -3.28680515e-01,\n",
       "        -4.69591320e-01,  5.70354342e-01],\n",
       "       [ 9.88768578e-01, -2.17444658e-01, -5.86301267e-01,\n",
       "         6.51117444e-01,  9.81271863e-01, -3.26828480e-01,\n",
       "        -4.79601562e-01,  5.88122964e-01],\n",
       "       [ 9.77269471e-01, -2.19228715e-01, -5.85249782e-01,\n",
       "         6.66152596e-01,  9.73197460e-01, -3.26123297e-01,\n",
       "        -4.52205360e-01,  5.65885425e-01]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input, train_output = create_inout_sequences(train_data_normalized, 98, 8, 20)\n",
    "test_input, test_output = create_inout_sequences(test_data_normalized, 98, 8, 20)\n",
    "\n",
    "train_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 20, 98)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(train_input.shape)\n",
    "train_data = TensorDataset(torch.from_numpy(train_input), torch.from_numpy(train_output))\n",
    "val_data = TensorDataset(torch.from_numpy(test_input), torch.from_numpy(test_output))\n",
    "test_data = TensorDataset(torch.from_numpy(test_input), torch.from_numpy(test_input))\n",
    "\n",
    "batch_size = 30\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = 98, hidden_dim = 200, batch_size = 30, output_dim=8,\n",
    "                    num_layers=2,drop_prob=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        batch_size = x.size(0)\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.linear(lstm_out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)    \n",
    "        # Only take the output from the final timetep\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "lr=0.005\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(98, 200, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 MSE:  0.31962230801582336\n",
      "Epoch  0 MSE:  0.21929769217967987\n",
      "Epoch  0 MSE:  0.33925867080688477\n",
      "Epoch  0 MSE:  0.1559327095746994\n",
      "Epoch  0 MSE:  0.20041418075561523\n",
      "Epoch  5 MSE:  0.04659449681639671\n",
      "Epoch  5 MSE:  0.051215760409832\n",
      "Epoch  5 MSE:  0.06098954379558563\n",
      "Epoch  5 MSE:  0.04552629962563515\n",
      "Epoch  5 MSE:  0.04653622955083847\n",
      "Epoch  10 MSE:  0.030654672533273697\n",
      "Epoch  10 MSE:  0.03186502307653427\n",
      "Epoch  10 MSE:  0.03768249228596687\n",
      "Epoch  10 MSE:  0.02762959524989128\n",
      "Epoch  10 MSE:  0.026601888239383698\n",
      "Epoch  15 MSE:  0.02357984147965908\n",
      "Epoch  15 MSE:  0.025942528620362282\n",
      "Epoch  15 MSE:  0.025603927671909332\n",
      "Epoch  15 MSE:  0.019770806655287743\n",
      "Epoch  15 MSE:  0.021830352023243904\n",
      "Epoch  20 MSE:  0.020172379910945892\n",
      "Epoch  20 MSE:  0.02336936444044113\n",
      "Epoch  20 MSE:  0.01971689611673355\n",
      "Epoch  20 MSE:  0.017794158309698105\n",
      "Epoch  20 MSE:  0.019630348309874535\n",
      "Epoch  25 MSE:  0.016387294977903366\n",
      "Epoch  25 MSE:  0.017248069867491722\n",
      "Epoch  25 MSE:  0.015203275717794895\n",
      "Epoch  25 MSE:  0.012576707638800144\n",
      "Epoch  25 MSE:  0.014577703550457954\n",
      "Epoch  30 MSE:  0.014625124633312225\n",
      "Epoch  30 MSE:  0.01355798076838255\n",
      "Epoch  30 MSE:  0.01280143577605486\n",
      "Epoch  30 MSE:  0.012294494546949863\n",
      "Epoch  30 MSE:  0.012249387800693512\n",
      "Epoch  35 MSE:  0.011499078944325447\n",
      "Epoch  35 MSE:  0.012000896036624908\n",
      "Epoch  35 MSE:  0.010780888609588146\n",
      "Epoch  35 MSE:  0.010826772078871727\n",
      "Epoch  35 MSE:  0.010577895678579807\n",
      "Epoch  40 MSE:  0.011248942464590073\n",
      "Epoch  40 MSE:  0.009714064188301563\n",
      "Epoch  40 MSE:  0.008240025490522385\n",
      "Epoch  40 MSE:  0.008114229887723923\n",
      "Epoch  40 MSE:  0.00941381137818098\n",
      "Epoch  45 MSE:  0.009353319182991982\n",
      "Epoch  45 MSE:  0.008606894873082638\n",
      "Epoch  45 MSE:  0.007350057829171419\n",
      "Epoch  45 MSE:  0.007287364918738604\n",
      "Epoch  45 MSE:  0.00781514123082161\n",
      "Epoch  50 MSE:  0.007626221980899572\n",
      "Epoch  50 MSE:  0.007118058390915394\n",
      "Epoch  50 MSE:  0.0077062081545591354\n",
      "Epoch  50 MSE:  0.007567283231765032\n",
      "Epoch  50 MSE:  0.0075662764720618725\n",
      "Epoch  55 MSE:  0.007576816249638796\n",
      "Epoch  55 MSE:  0.006050340365618467\n",
      "Epoch  55 MSE:  0.006166440434753895\n",
      "Epoch  55 MSE:  0.006724762264639139\n",
      "Epoch  55 MSE:  0.007118470035493374\n",
      "Epoch  60 MSE:  0.0062353950925171375\n",
      "Epoch  60 MSE:  0.005529543850570917\n",
      "Epoch  60 MSE:  0.005022337194532156\n",
      "Epoch  60 MSE:  0.005634930916130543\n",
      "Epoch  60 MSE:  0.005658692680299282\n",
      "Epoch  65 MSE:  0.006501007825136185\n",
      "Epoch  65 MSE:  0.005313126835972071\n",
      "Epoch  65 MSE:  0.004604022484272718\n",
      "Epoch  65 MSE:  0.00520058861002326\n",
      "Epoch  65 MSE:  0.005236773286014795\n",
      "Epoch  70 MSE:  0.005022445693612099\n",
      "Epoch  70 MSE:  0.0042808158323168755\n",
      "Epoch  70 MSE:  0.00456690089777112\n",
      "Epoch  70 MSE:  0.005188075825572014\n",
      "Epoch  70 MSE:  0.0051178704015910625\n",
      "Epoch  75 MSE:  0.004951065871864557\n",
      "Epoch  75 MSE:  0.003680572612211108\n",
      "Epoch  75 MSE:  0.003769442206248641\n",
      "Epoch  75 MSE:  0.004542945884168148\n",
      "Epoch  75 MSE:  0.00471203587949276\n",
      "Epoch  80 MSE:  0.004441870376467705\n",
      "Epoch  80 MSE:  0.003609325038269162\n",
      "Epoch  80 MSE:  0.003960250411182642\n",
      "Epoch  80 MSE:  0.0042218915186822414\n",
      "Epoch  80 MSE:  0.005333470646291971\n",
      "Epoch  85 MSE:  0.004458538722246885\n",
      "Epoch  85 MSE:  0.003666599513962865\n",
      "Epoch  85 MSE:  0.00417675357311964\n",
      "Epoch  85 MSE:  0.0038657633122056723\n",
      "Epoch  85 MSE:  0.004084978252649307\n",
      "Epoch  90 MSE:  0.003972196951508522\n",
      "Epoch  90 MSE:  0.0031497804448008537\n",
      "Epoch  90 MSE:  0.0035474023316055536\n",
      "Epoch  90 MSE:  0.003440659726038575\n",
      "Epoch  90 MSE:  0.004207600839436054\n",
      "Epoch  95 MSE:  0.003731486853212118\n",
      "Epoch  95 MSE:  0.0029693429823964834\n",
      "Epoch  95 MSE:  0.002938054036349058\n",
      "Epoch  95 MSE:  0.0032348204404115677\n",
      "Epoch  95 MSE:  0.0033239240292459726\n",
      "Epoch  100 MSE:  0.0033344158437103033\n",
      "Epoch  100 MSE:  0.0030286144465208054\n",
      "Epoch  100 MSE:  0.0033873487263917923\n",
      "Epoch  100 MSE:  0.00350804440677166\n",
      "Epoch  100 MSE:  0.003060247516259551\n",
      "Epoch  105 MSE:  0.0031500039622187614\n",
      "Epoch  105 MSE:  0.002547933254390955\n",
      "Epoch  105 MSE:  0.002525504445657134\n",
      "Epoch  105 MSE:  0.0029014532919973135\n",
      "Epoch  105 MSE:  0.0030902219004929066\n",
      "Epoch  110 MSE:  0.0030753398314118385\n",
      "Epoch  110 MSE:  0.002265223069116473\n",
      "Epoch  110 MSE:  0.002696908311918378\n",
      "Epoch  110 MSE:  0.0027949197683483362\n",
      "Epoch  110 MSE:  0.0028443399351090193\n",
      "Epoch  115 MSE:  0.0031677072402089834\n",
      "Epoch  115 MSE:  0.0023066564463078976\n",
      "Epoch  115 MSE:  0.0026322370395064354\n",
      "Epoch  115 MSE:  0.002936502220109105\n",
      "Epoch  115 MSE:  0.0027449934277683496\n",
      "Epoch  120 MSE:  0.0030018421821296215\n",
      "Epoch  120 MSE:  0.00212110229767859\n",
      "Epoch  120 MSE:  0.0022421993780881166\n",
      "Epoch  120 MSE:  0.0023305534850806\n",
      "Epoch  120 MSE:  0.0027569574303925037\n",
      "Epoch  125 MSE:  0.0028556068427860737\n",
      "Epoch  125 MSE:  0.0023048035800457\n",
      "Epoch  125 MSE:  0.0023246193304657936\n",
      "Epoch  125 MSE:  0.0023353942669928074\n",
      "Epoch  125 MSE:  0.0023353660944849253\n",
      "Epoch  130 MSE:  0.002819369314238429\n",
      "Epoch  130 MSE:  0.002529901685193181\n",
      "Epoch  130 MSE:  0.0019267300376668572\n",
      "Epoch  130 MSE:  0.0025157665368169546\n",
      "Epoch  130 MSE:  0.002307355869561434\n",
      "Epoch  135 MSE:  0.0025099210906773806\n",
      "Epoch  135 MSE:  0.0018727938877418637\n",
      "Epoch  135 MSE:  0.0019910086411982775\n",
      "Epoch  135 MSE:  0.0021198661997914314\n",
      "Epoch  135 MSE:  0.0021866527386009693\n",
      "Epoch  140 MSE:  0.0024465592578053474\n",
      "Epoch  140 MSE:  0.0017364990198984742\n",
      "Epoch  140 MSE:  0.002010828349739313\n",
      "Epoch  140 MSE:  0.002073417417705059\n",
      "Epoch  140 MSE:  0.0020464507397264242\n",
      "Epoch  145 MSE:  0.0026136410888284445\n",
      "Epoch  145 MSE:  0.0017422862583771348\n",
      "Epoch  145 MSE:  0.002211853628978133\n",
      "Epoch  145 MSE:  0.0020547471940517426\n",
      "Epoch  145 MSE:  0.002104242332279682\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "counter = 0\n",
    "print_every =50\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "\n",
    "hist = np.zeros(epochs)\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "    # Forward pass\n",
    "        y_pred, model.hidden = model(inputs, model.hidden)\n",
    "        labels = labels.view(batch_size,-1)\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        if t % 5 == 0:\n",
    "            print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "        hist[t] = loss.item()\n",
    "    \n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "    \n",
    "    # Backward pass\n",
    "        loss.backward(retain_graph=True)\n",
    "    \n",
    "    # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.train()\n",
    "# for i in range(epochs):\n",
    "# #     h = model.init_hidden(batch_size)\n",
    "    \n",
    "#     for inputs, labels in train_loader:\n",
    "#         counter += 1\n",
    "#         optimizer.zero_grad()\n",
    "#         h = (torch.zeros(2, batch_size, model.hidden_dim).to(device),\n",
    "#                         torch.zeros(2, batch_size, model.hidden_dim).to(device))\n",
    "#         h = tuple([e.data for e in h])\n",
    "#         model.zero_grad()\n",
    "#         output, h = model(inputs, h)\n",
    "#         print(output)\n",
    "#         loss = criterion(output, labels.view(batch_size,-1))\n",
    "#         loss.backward()\n",
    "#         print(loss)\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "#         print(counter)\n",
    "        \n",
    "        \n",
    "#         if counter%print_every == 0:\n",
    "#             val_h = model.init_hidden(batch_size)\n",
    "#             val_losses = []\n",
    "#             model.eval()\n",
    "#             for inp, lab in val_loader:\n",
    "#                 val_h = tuple([each.data for each in val_h])\n",
    "#                 inp, lab = inp.to(device), lab.to(device)\n",
    "#                 out, val_h = model(inp, val_h)\n",
    "#                 val_loss = criterion(out.squeeze(), lab.float())\n",
    "#                 val_losses.append(val_loss.item())\n",
    "                \n",
    "#             model.train()\n",
    "#             print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "#                   \"Step: {}...\".format(counter),\n",
    "#                   \"Loss: {:.6f}...\".format(loss.item()),\n",
    "#                   \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "#             if np.mean(val_losses) <= valid_loss_min:\n",
    "#                 torch.save(model.state_dict(), './state_dict.pt')\n",
    "#                 print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "#                 valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
