{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from training & testing dataset\n",
    "training_csv_file = 'dataset_2020_training.csv'\n",
    "testing_csv_file = 'dataset_2020_testing1.csv'\n",
    "training_set = pd.read_csv(training_csv_file, header=0)\n",
    "testing_set = pd.read_csv(testing_csv_file, header=0)\n",
    "\n",
    "#The output is: knee/ankle angle/torque l/r\n",
    "out_num = 8\n",
    "\n",
    "#Get the label\n",
    "Label_df = pd.read_csv(training_csv_file, header=0, nrows = 1)\n",
    "Label= Label_df.columns.values\n",
    "output_label = np.array(Label[-out_num:])\n",
    "\n",
    "#Get the values and convert to float type\n",
    "training_values = training_set.values\n",
    "testing_values = testing_set.values\n",
    "time_train = training_values[:,0]\n",
    "time_test  = testing_values[:,0]\n",
    "training_data = training_values[:,1:].astype('float32')\n",
    "testing_data = testing_values[:,1:].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(ndarray):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler.fit(ndarray)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get scaler for inputs/outputs\n",
    "X_scaler_training = get_scaler(training_data[:,:98])\n",
    "Y_scaler_training = get_scaler(training_data[:,-8:])\n",
    "X_scaler_testing = get_scaler(testing_data[:,:98])\n",
    "Y_scaler_testing = get_scaler(testing_data[:,-8:])\n",
    "\n",
    "\n",
    "X_train = X_scaler_training.transform(training_data[:,:98])\n",
    "Y_train = Y_scaler_training.transform(training_data[:,-8:])\n",
    "X_test = X_scaler_testing.transform(testing_data[:,:98])\n",
    "Y_test = Y_scaler_testing.transform(testing_data[:,-8:])\n",
    "\n",
    "#Get scaler for the whole training data\n",
    "train_data_scaler = get_scaler(training_data)\n",
    "test_data_scaler = get_scaler(testing_data)\n",
    "\n",
    "#Transform training data into scaled data and tensor data type\n",
    "train_data_normalized = train_data_scaler.transform(training_data)\n",
    "test_data_normalized =test_data_scaler.transform(testing_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(A, timesteps):\n",
    "    return A.reshape(int(A.shape[0] / timesteps), timesteps, A.shape[1])\n",
    "\n",
    "#Return tupple containing inputs and labels\n",
    "def create_inout_sequences(input_data, size, out_num, time_steps):\n",
    "    seq = reshape(input_data[:,:size], time_steps)\n",
    "    out = reshape(input_data[:,-out_num:], time_steps)\n",
    "    return seq, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-5.56939244e-01,  7.11062193e-01,  4.09192979e-01,\n",
       "         1.07450366e-01, -5.53563714e-01,  7.01500356e-01,\n",
       "         5.65648794e-01,  4.27066684e-02],\n",
       "       [-6.78051233e-01,  7.79404044e-01,  5.25343478e-01,\n",
       "        -4.01988029e-02, -6.72574282e-01,  7.77232230e-01,\n",
       "         6.33226871e-01, -1.05120778e-01],\n",
       "       [-7.62396812e-01,  8.24274182e-01,  5.89141190e-01,\n",
       "        -1.71931505e-01, -7.54591703e-01,  8.27659547e-01,\n",
       "         6.53589964e-01, -1.95789218e-01],\n",
       "       [-8.07320595e-01,  8.48463893e-01,  6.36925519e-01,\n",
       "        -3.51417303e-01, -7.98710585e-01,  8.53516400e-01,\n",
       "         6.54876828e-01, -3.51654768e-01],\n",
       "       [-8.13657999e-01,  8.60693097e-01,  6.54800475e-01,\n",
       "        -4.82574940e-01, -8.03837299e-01,  8.60979736e-01,\n",
       "         6.32220268e-01, -5.01585841e-01],\n",
       "       [-7.80356884e-01,  8.62670898e-01,  6.19420826e-01,\n",
       "        -4.96664524e-01, -7.67527580e-01,  8.52330148e-01,\n",
       "         5.67536354e-01, -5.28172612e-01],\n",
       "       [-7.05393553e-01,  8.44888091e-01,  5.47699630e-01,\n",
       "        -4.31240082e-01, -6.89166307e-01,  8.23829234e-01,\n",
       "         4.86198306e-01, -4.87417102e-01],\n",
       "       [-5.88831067e-01,  8.02520752e-01,  4.44339216e-01,\n",
       "        -3.17683935e-01, -5.69055676e-01,  7.71836579e-01,\n",
       "         4.21741247e-01, -3.60556006e-01],\n",
       "       [-4.30692554e-01,  7.27441549e-01,  3.19305360e-01,\n",
       "        -1.45211935e-01, -4.07251835e-01,  6.81896508e-01,\n",
       "         2.93547213e-01, -1.44286394e-01],\n",
       "       [-2.31594443e-01,  6.08628273e-01,  5.42485714e-02,\n",
       "         8.55844021e-02, -2.04840183e-01,  5.45670569e-01,\n",
       "         2.09428072e-02,  1.63272619e-01],\n",
       "       [ 1.54614449e-04,  4.37458754e-01, -3.57268929e-01,\n",
       "         3.70382994e-01,  2.94506550e-02,  3.58753264e-01,\n",
       "        -3.09730411e-01,  4.95305777e-01],\n",
       "       [ 2.51327634e-01,  2.35927343e-01, -6.32676303e-01,\n",
       "         5.79616249e-01,  2.80932546e-01,  1.41518176e-01,\n",
       "        -4.87587422e-01,  6.88547254e-01],\n",
       "       [ 4.94673312e-01,  3.98510695e-02, -7.06878662e-01,\n",
       "         6.56009197e-01,  5.18931568e-01, -7.06242919e-02,\n",
       "        -5.47403038e-01,  7.43945718e-01],\n",
       "       [ 7.00899005e-01, -1.11531496e-01, -7.39466906e-01,\n",
       "         6.59382582e-01,  7.13588417e-01, -2.39833772e-01,\n",
       "        -5.92887998e-01,  6.89375281e-01],\n",
       "       [ 8.49679351e-01, -1.93062603e-01, -6.11851096e-01,\n",
       "         6.21005774e-01,  8.48834157e-01, -3.28465104e-01,\n",
       "        -5.44212759e-01,  5.30233502e-01],\n",
       "       [ 9.33025062e-01, -2.13918567e-01, -5.48408866e-01,\n",
       "         5.83273530e-01,  9.21841860e-01, -3.41141164e-01,\n",
       "        -4.88733858e-01,  4.51241255e-01],\n",
       "       [ 9.74897504e-01, -2.11151153e-01, -5.98715961e-01,\n",
       "         5.71254015e-01,  9.56764698e-01, -3.31867695e-01,\n",
       "        -4.60030973e-01,  5.21491647e-01],\n",
       "       [ 9.92575169e-01, -2.12029338e-01, -5.99507928e-01,\n",
       "         6.11360192e-01,  9.76594269e-01, -3.28680515e-01,\n",
       "        -4.69591320e-01,  5.70354342e-01],\n",
       "       [ 9.88768578e-01, -2.17444658e-01, -5.86301267e-01,\n",
       "         6.51117444e-01,  9.81271863e-01, -3.26828480e-01,\n",
       "        -4.79601562e-01,  5.88122964e-01],\n",
       "       [ 9.77269471e-01, -2.19228715e-01, -5.85249782e-01,\n",
       "         6.66152596e-01,  9.73197460e-01, -3.26123297e-01,\n",
       "        -4.52205360e-01,  5.65885425e-01]], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input, train_output = create_inout_sequences(train_data_normalized, 98, 8, 20)\n",
    "test_input, test_output = create_inout_sequences(test_data_normalized, 98, 8, 20)\n",
    "\n",
    "train_output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_input), torch.from_numpy(train_output))\n",
    "val_data = TensorDataset(torch.from_numpy(test_input), torch.from_numpy(test_output))\n",
    "test_data = TensorDataset(torch.from_numpy(test_input), torch.from_numpy(test_input))\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define LSTM class\n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim = 98, hidden_dim = 200, batch_size = 60, output_dim=8,\n",
    "                    num_layers=2,drop_prob=0.5):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers, dropout=drop_prob)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # Define the output layer\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        # This is what we'll initialise our hidden state as\n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Forward pass through LSTM layer\n",
    "        # shape of lstm_out: [input_size, batch_size, hidden_dim]\n",
    "        # shape of self.hidden: (a, b), where a and b both \n",
    "        # have shape (num_layers, batch_size, hidden_dim).\n",
    "        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))\n",
    "        \n",
    "        # Only take the output from the final timetep\n",
    "        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction\n",
    "        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))\n",
    "        return y_pred.view(-1), self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM()\n",
    "lr=0.005\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(98, 200, num_layers=2, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (linear): Linear(in_features=200, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-143-966901d9bfc6>, line 24)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-143-966901d9bfc6>\"\u001b[1;36m, line \u001b[1;32m24\u001b[0m\n\u001b[1;33m    print(\"Epoch \", t, \"MSE: \", loss.item())\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every =50\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "\n",
    "hist = np.zeros(epochs)\n",
    "\n",
    "for t in range(epochs):\n",
    "    # Clear stored gradient\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    model.hidden = model.init_hidden()\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "    # Forward pass\n",
    "        y_pred = model(inputs)\n",
    "\n",
    "        loss = loss_fn(y_pred, labels)\n",
    "        if t % 100 == 0:\n",
    "        print(\"Epoch \", t, \"MSE: \", loss.item())\n",
    "        hist[t] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "        optimiser.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model.train()\n",
    "# for i in range(epochs):\n",
    "# #     h = model.init_hidden(batch_size)\n",
    "    \n",
    "#     for inputs, labels in train_loader:\n",
    "#         counter += 1\n",
    "#         optimizer.zero_grad()\n",
    "#         h = (torch.zeros(2, batch_size, model.hidden_dim).to(device),\n",
    "#                         torch.zeros(2, batch_size, model.hidden_dim).to(device))\n",
    "#         h = tuple([e.data for e in h])\n",
    "#         model.zero_grad()\n",
    "#         output, h = model(inputs, h)\n",
    "#         print(output)\n",
    "#         loss = criterion(output, labels.view(batch_size,-1))\n",
    "#         loss.backward()\n",
    "#         print(loss)\n",
    "#         nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "#         optimizer.step()\n",
    "#         print(counter)\n",
    "        \n",
    "        \n",
    "#         if counter%print_every == 0:\n",
    "#             val_h = model.init_hidden(batch_size)\n",
    "#             val_losses = []\n",
    "#             model.eval()\n",
    "#             for inp, lab in val_loader:\n",
    "#                 val_h = tuple([each.data for each in val_h])\n",
    "#                 inp, lab = inp.to(device), lab.to(device)\n",
    "#                 out, val_h = model(inp, val_h)\n",
    "#                 val_loss = criterion(out.squeeze(), lab.float())\n",
    "#                 val_losses.append(val_loss.item())\n",
    "                \n",
    "#             model.train()\n",
    "#             print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "#                   \"Step: {}...\".format(counter),\n",
    "#                   \"Loss: {:.6f}...\".format(loss.item()),\n",
    "#                   \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "#             if np.mean(val_losses) <= valid_loss_min:\n",
    "#                 torch.save(model.state_dict(), './state_dict.pt')\n",
    "#                 print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "#                 valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
